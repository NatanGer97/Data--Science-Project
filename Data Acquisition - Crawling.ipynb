{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition - Crawling\n",
    "*We acquire our data from* [MetaCritic Best Game of All Time](https://www.metacritic.com/browse/games/score/metascore/all/all/filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from collections import defaultdict\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dictionary for all the info we are scraping from MetaCritic for each game**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {\"Index\":[],\"Title\":[],\"Release Date\":[],\"Platform\":[],\"MetaScore\":[],\"NumberOfCritics\":[],\"PositiveMetaScore\":[],\"MixedMetaScore\":[],\"NegativeMetaScore\":[]\n",
    ",\"UserScore\":[],\"NumberOfUsers\":[],\"PositiveUserScore\":[],\"MixedUserScore\":[],\"NegativeUserScore\":[],\"Awards\":[],\"Rating\":[],\"OfficialSite\":[],\"Developer\":[],\"# Players\":[],\"Genres\":[]\n",
    ",\"ESRB Descriptors\":[],\"Connectivity\":[],\"# Online Players\":[],\"OnlineModes\":[],\"OfflineModes\":[],\"Resolution\":[],\"SpecialControllers\":[],\"Sound\":[]\n",
    ",\"Compatibility\":[],\"Customization\":[],\"SplitScreenOnlinePlayers\":[],\"SplitScreenOfflinePlayers\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageURL = \"/browse/games/score/metascore/all/all/filtered\"\n",
    "baseURL = \"https://www.metacritic.com\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\"}\n",
    "response = requests.get(baseURL + pageURL ,headers=headers)\n",
    "soup = BeautifulSoup(response.content ,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting details from the games detail table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillGameDetails(soupForGamePage,detailsDict):\n",
    "    try:\n",
    "        time.sleep(0.2)\n",
    "        gameDetailsURL = baseURL + soupForGamePage.find('div',attrs={'id':'main','class':'col main_col'}).find('li',attrs={'class':'nav nav_details'}).find('a').attrs['href']\n",
    "        responseForGameDetails = requests.get(gameDetailsURL ,headers=headers)\n",
    "        soupForGameDetails = BeautifulSoup(responseForGameDetails.content ,'html.parser')\n",
    "        gameDetailsDataTable = soupForGameDetails.find('div',attrs={'id':'main','class':'col main_col'}).find_all('div',attrs={'class':'product_details'})[1].find_all('tr')\n",
    "        getTextFromTable(gameDetailsDataTable,detailsDict)\n",
    "        return True\n",
    "    except AttributeError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The attributes in the table is different for each game so we decided to build a list with all the possible attributes for \n",
    "#any game and to check \"manually\" if the attribute is present , if not - adding NaN instead\n",
    "\n",
    "def getTextFromTable(tableSoup,detailsDict):\n",
    "    columnsToAddNaN = [\"Rating\",\"OfficialSite\",\"Developer\",\"# Players\",\"Genres\",\"ESRB Descriptors\",\"Connectivity\"\n",
    "    ,\"# Online Players\",\"OnlineModes\",\"OfflineModes\",\"Resolution\",\"SpecialControllers\",\"Sound\",\"Compatibility\"\n",
    "    ,\"Customization\",\"SplitScreenOnlinePlayers\",\"SplitScreenOfflinePlayers\"]\n",
    "    for rowInTable in tableSoup:\n",
    "        if rowInTable.find('th').get_text() ==  \"Genre(s):\":\n",
    "            detailsDict[\"Genres\"].append(rowInTable.find('td').get_text().replace(\" \",\"\").replace(\"\\r\",\"\").replace(\"\\n\",\"\"))\n",
    "            columnsToAddNaN.remove(\"Genres\")\n",
    "            continue\n",
    "        \n",
    "        if rowInTable.find('th').get_text() == \"Rating:\":\n",
    "            detailsDict[\"Rating\"].append(rowInTable.find('td').get_text())\n",
    "            columnsToAddNaN.remove(\"Rating\")\n",
    "            continue\n",
    "\n",
    "        if rowInTable.find('th').get_text() == \"Developer:\":\n",
    "            detailsDict[\"Developer\"].append(rowInTable.find('td').get_text())\n",
    "            columnsToAddNaN.remove(\"Developer\")\n",
    "            continue\n",
    "\n",
    "        if rowInTable.find('th').get_text() == \"Official Site:\":\n",
    "            detailsDict[\"OfficialSite\"].append(rowInTable.find('td').get_text())\n",
    "            columnsToAddNaN.remove(\"OfficialSite\")\n",
    "            continue\n",
    "        \n",
    "        if rowInTable.find('th').get_text() == \"Connectivity:\":\n",
    "            detailsDict[\"Connectivity\"].append(rowInTable.find('td').get_text())\n",
    "            columnsToAddNaN.remove(\"Connectivity\")\n",
    "            continue\n",
    "\n",
    "        if rowInTable.find('th').get_text() == \"Customization\":\n",
    "            detailsDict[\"Customization\"].append(rowInTable.find('td').get_text())\n",
    "            columnsToAddNaN.remove(\"Customization\")\n",
    "            continue\n",
    "\n",
    "        if rowInTable.find('th').get_text() == \"ESRB Descriptors:\":\n",
    "            detailsDict[\"ESRB Descriptors\"].append(rowInTable.find('td').get_text())\n",
    "            columnsToAddNaN.remove(\"ESRB Descriptors\")\n",
    "            continue\n",
    "\n",
    "        if rowInTable.find('th').get_text() == \"Number of Online Players:\t\":\n",
    "            detailsDict[\"# Online Players\"].append(rowInTable.find('td').get_text())\n",
    "            columnsToAddNaN.remove(\"# Online Players\")\n",
    "            continue\n",
    "\n",
    "        if rowInTable.find('th').get_text() == \"Number of Players:\":\n",
    "            detailsDict[\"# Players\"].append(rowInTable.find('td').get_text())\n",
    "            columnsToAddNaN.remove(\"# Players\")\n",
    "            continue\n",
    "\n",
    "        if rowInTable.find('th').get_text() == \"Online Modes:\":\n",
    "            detailsDict[\"OnlineModes\"].append(rowInTable.find('td').get_text())\n",
    "            columnsToAddNaN.remove(\"OnlineModes\")\n",
    "            continue\n",
    "\n",
    "        if rowInTable.find('th').get_text() == \"Offline Modes:\":\n",
    "            detailsDict[\"OfflineModes\"].append(rowInTable.find('td').get_text())\n",
    "            columnsToAddNaN.remove(\"OfflineModes\")\n",
    "            continue\n",
    "\n",
    "        if rowInTable.find('th').get_text() == \"Resolution:\":\n",
    "            detailsDict[\"Resolution\"].append(rowInTable.find('td').get_text())\n",
    "            columnsToAddNaN.remove(\"Resolution\")\n",
    "            continue\n",
    "\n",
    "        if rowInTable.find('th').get_text() == \"Sound\":\n",
    "            detailsDict[\"Sound\"].append(rowInTable.find('td').get_text())\n",
    "            columnsToAddNaN.remove(\"Sound\")\n",
    "            continue\n",
    "\n",
    "        if rowInTable.find('th').get_text() == \"Special Controllers:\":\n",
    "            detailsDict[\"SpecialControllers\"].append(rowInTable.find('td').get_text())\n",
    "            columnsToAddNaN.remove(\"SpecialControllers\")\n",
    "            continue\n",
    "        \n",
    "        if rowInTable.find('th').get_text() == \"Split Screen Online Players:\":\n",
    "            detailsDict[\"SplitScreenOnlinePlayers\"].append(rowInTable.find('td').get_text())\n",
    "            columnsToAddNaN.remove(\"SplitScreenOnlinePlayers\")\n",
    "            continue\n",
    "\n",
    "        if rowInTable.find('th').get_text() == \"Split Screen Offline Players:\":\n",
    "            detailsDict[\"SplitScreenOfflinePlayers\"].append(rowInTable.find('td').get_text())\n",
    "            columnsToAddNaN.remove(\"Sound\")\n",
    "            continue\n",
    "\n",
    "        if rowInTable.find('th').get_text() == \"Compatibility\":\n",
    "            detailsDict[\"Compatibility\"].append(rowInTable.find('td').get_text())\n",
    "            columnsToAddNaN.remove(\"Compatibility\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "    for colName in columnsToAddNaN:\n",
    "        detailsDict[colName].append(\"NaN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acquiring MetaScore and Score attribute and their info:**\n",
    "- MetaScore:\n",
    "    - Number of critics\n",
    "    - Amount of positive reviews\n",
    "    - Amount of mixed reviews\n",
    "    - Amount of negative reviews\n",
    "- UserScore:\n",
    "    - Number of users\n",
    "    - Amount of positive reviews\n",
    "    - Amount of mixed reviews\n",
    "    - Amount of negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScoreDetails(soupForGamePage, dict):\n",
    "    time.sleep(0.2)\n",
    "    #Getting 2 urls , First is for Metascore review page and the Second is for userscore review page\n",
    "    metascoreURL = baseURL + soupForGamePage.find('div',attrs={'class':\"details main_details\"}).find('a').attrs['href']\n",
    "    userscoreURL = baseURL + soupForGamePage.find('div',attrs={'class':\"details side_details\"}).find('a').attrs['href']\n",
    "\n",
    "    #Getting metascore details : \n",
    "    responseFromMetascorePage = requests.get(metascoreURL ,headers=headers)\n",
    "    soupForMetaScoreDetails = BeautifulSoup(responseFromMetascorePage.content ,'html.parser')\n",
    "    scoreWrapper = soupForMetaScoreDetails.find('div',attrs={'class':'module score_details_module'})\n",
    "    try:\n",
    "        dict[\"MetaScore\"].append(scoreWrapper.find('div',attrs={'class': lambda e: e.startswith('metascore_w') if e else False}).find('span').get_text())\n",
    "        dict[\"NumberOfCritics\"].append(scoreWrapper.find('strong').get_text().strip())\n",
    "        dict[\"PositiveMetaScore\"].append(scoreWrapper.find_all('li',attrs={'class':'score_count'})[0].find('span',attrs={'class':'count'}).get_text())\n",
    "        dict[\"MixedMetaScore\"].append(scoreWrapper.find_all('li',attrs={'class':'score_count'})[1].find('span',attrs={'class':'count'}).get_text())\n",
    "        dict[\"NegativeMetaScore\"].append(scoreWrapper.find_all('li',attrs={'class':'score_count'})[2].find('span',attrs={'class':'count'}).get_text())\n",
    "    except AttributeError:\n",
    "        # The data from the site is missing so we use the value from the game before because the score should be identical\n",
    "        dict[\"MetaScore\"].append(dict[\"MetaScore\"][-1])\n",
    "        dict[\"NumberOfCritics\"].append(\"NaN\")\n",
    "        dict[\"PositiveMetaScore\"].append(\"NaN\")\n",
    "        dict[\"MixedMetaScore\"].append(\"NaN\")\n",
    "        dict[\"NegativeMetaScore\"].append(\"NaN\")\n",
    "\n",
    "    #Getting userscore details :\n",
    "    responseForUserScorePage = requests.get(userscoreURL ,headers=headers)\n",
    "    soupForUserScoreDetails = BeautifulSoup(responseForUserScorePage.content ,'html.parser')\n",
    "    scoreWrapper2 = soupForUserScoreDetails.find('div',attrs={'class':'module score_details_module'})\n",
    "    try:\n",
    "        dict[\"UserScore\"].append(scoreWrapper2.find('div',attrs={'class': lambda e: e.startswith('metascore_w user') if e else False}).get_text())\n",
    "        dict[\"NumberOfUsers\"].append(scoreWrapper2.find('strong').get_text().strip())\n",
    "        dict[\"PositiveUserScore\"].append(scoreWrapper2.find_all('li',attrs={'class':'score_count'})[0].find('span',attrs={'class':'count'}).get_text())\n",
    "        dict[\"MixedUserScore\"].append(scoreWrapper2.find_all('li',attrs={'class':'score_count'})[1].find('span',attrs={'class':'count'}).get_text())\n",
    "        dict[\"NegativeUserScore\"].append(scoreWrapper2.find_all('li',attrs={'class':'score_count'})[2].find('span',attrs={'class':'count'}).get_text())\n",
    "    except AttributeError:\n",
    "        # This time we can't use the value from the game before this one because the values can be different\n",
    "        dict[\"UserScore\"].append(\"NaN\")\n",
    "        dict[\"NumberOfUsers\"].append(\"NaN\")\n",
    "        dict[\"PositiveUserScore\"].append(\"NaN\")\n",
    "        dict[\"MixedUserScore\"].append(\"NaN\")\n",
    "        dict[\"NegativeUserScore\"].append(\"NaN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acquiring Awards attribute**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAwardAndRankings(soupForGamePage, dict):\n",
    "    awardsList = []\n",
    "    try:\n",
    "        awardsAndRankingsBoxWrapper = soupForGamePage.find('div',attrs={'class':'module list_rankings contain_module'})\n",
    "        for tableRow in awardsAndRankingsBoxWrapper.find('div',attrs={'class':'body'}).find('table',attrs={'class':'rankings'}).find_all('div',attrs={'class':'ranking_title'}):\n",
    "            awardTitle = tableRow.get_text().strip()\n",
    "            awardsList.append(awardTitle)\n",
    "        #The game didn't win any awards or special rankings\n",
    "        if not awardsList:\n",
    "            dict[\"Awards\"].append(\"NaN\")\n",
    "        #The game won awards or has special rankings\n",
    "        else:\n",
    "            dict[\"Awards\"].append(\" || \".join(awardsList))\n",
    "    except AttributeError:\n",
    "        dict[\"Awards\"].append(\"NaN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main code cell of the crawling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are 191 pages in total , 100 Games for each page\n",
    "for i in range(191):\n",
    "    contentWarper = soup.find('div',attrs={'class':'title_bump'})\n",
    "    browse_list_wrapperElements = contentWarper.find_all('div',attrs={'class':'browse_list_wrapper'})\n",
    "\n",
    "    #Each browse_list_wrapperElements contains 100 games\n",
    "    for item in browse_list_wrapperElements:\n",
    "        game = item.find_all('td',attrs={'class':'clamp-summary-wrap'})\n",
    "        for i in range(len(game)):\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                gamePageURL = baseURL + game[i].find('a',attrs={'class':'title'}).attrs['href']\n",
    "                responseForGamePage = requests.get(gamePageURL ,headers=headers)\n",
    "                soupForGamePage = BeautifulSoup(responseForGamePage.content ,'html.parser')\n",
    "        \n",
    "                #Scraping for Scores and Award attributes\n",
    "                getScoreDetails(soupForGamePage, dict)\n",
    "                getAwardAndRankings(soupForGamePage, dict)\n",
    "\n",
    "                # After some time of scraping the site started to return \"blank pages\" , so we decided to try at least 10 times before giving up on the game,\n",
    "                # altough after a day of break from scraping thorugh the site there was no blank pages returned anymore , but we still decided to keep the code.\n",
    "                numberOfTries = 0\n",
    "                while not fillGameDetails(soupForGamePage ,dict) and numberOfTries != 10:\n",
    "                   numberOfTries+=1\n",
    "                if(numberOfTries != 10):\n",
    "                   dict[\"Index\"].append(game[i].find('span',attrs={'class':'title numbered'}).get_text().strip())\n",
    "                   dict[\"Title\"].append(game[i].find('h3').get_text())\n",
    "                   dict[\"Platform\"].append(game[i].find('span',attrs={'class':'data'}).text.strip())\n",
    "                   dict[\"Release Date\"].append(game[i].find('div',attrs={'class':'clamp-details'}).find_all('span')[-1].get_text())\n",
    "\n",
    "            except AttributeError:\n",
    "                continue\n",
    "\n",
    "    # Go through the next 100 games in the next page       \n",
    "    pageURL = contentWarper.find('a',attrs={\"class\":\"action\",\"rel\":\"next\"}).attrs['href']\n",
    "    response = requests.get(baseURL + pageURL ,headers=headers)\n",
    "    soup = BeautifulSoup(response.content ,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the dataframe after acquiring the necessary data and writing it to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = pd.DataFrame(dict)\n",
    "dataFrame.replace(to_replace=\"NaN\",value=np.nan,inplace=True)\n",
    "dataFrame.to_csv(\"GamesDataFrame.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "145dce410f627c3f4bace442f30927e7d934fd52308d51a13091780548fd05f7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
